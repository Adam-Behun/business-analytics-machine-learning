# Support Vector Machine


# Support Vector Machine
- comes from classification
- draw a line that distinguishes between the points - divide the entire domain into two parts
- Optimal hyperplane, support vectors are the closest points from the different classes, maximised margin
- the only points that matter are the ones closest to each other from different classes
- think about the number of variables and how the space looks like. How many dimensions do we have
- 20 independent variables and 20 dimensions - complex space needs to be divided into 2 sets
- what if you cannot use a straight line to divide the space
  - Use Kernel - math function -- increasing the problem dimension by one. 
  - Create a third dimension to create decision surface
  - Then, use an optimal hyperplane to divide the space

Bell-shaped and other shaped kernels
- Linear
- 2.nd polynomial
- 3.rd polynomial
- radial basis
- sigmoid


When using support vector machine, try different kernels

Google using SVM to see what is spam and what is not
- presence of keywords, or who is the sender

Amazon product recommendation ML models

Understand the basics of the program, and the algorithm --> know the why

## Exercise 

Run iris2.csv and predict the Petal.Width column for the last 5 species

Use stasbuddy and all learned ML models that can solve a regression model

Linear Regression
Decision Tree
Random Forest
Support Vector Machine


# Naive Bayes

- fast algorithm using probabilities, not as accurate as random forest
- used where real-time is needed - customer interaction
- theory of probability, conditional probability
- calculate the conditional probabilities of independent variables



In 2014
Extreme radient boosting
Before the best was random forest but it is relatively slow