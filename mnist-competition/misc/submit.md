# Introduction 
The third challenge we participated in during the class Business Analytics with Machine Learning was focused around the MNIST ("Modified National Institute of Standards and Technology") dataset. This dataset consists of hand-written images of zero through nine digits and is well-known as the introduction to computer vision, however, this problem can be also solved as a classification problem. Deep learning and supervised machine learning techniques were the two paths that I have decided to look into: simple neural network (my best score) and classification methods before which I used dimensionality reduction techniques such as Principal Component Analysis and t-SNE. I have tried different algorithms and dimensionality reduction techniques and my best score was achieved using a neural network, however, our class is more focused on supervised machine learning and so I will only briefly mention my implementation of the neural network strategy and after that I will focus mostly on the supervised machine learning solution of this problem. 

# Neural Network Implementation
To implement the neural network, I first imported some python libraries into my Jupyter notebook. I have used deep learning framework TensorFlow and Keras. Then I read the two datasets into their respective pandas data frames. Then, using the seaborn library, I visualized the distribution of the target variable "label" in the training dataset. Each bar in the graph represented the number of occurrences of each unique value in the dataset. Then, normalizing the data = the value of each pixel in an image is represented by an integer between 0 and 255. Normalization puts all the values between zero and one. This is useful because it can help our machine learning model to be faster and more accurate. Then, I used one-hot encoding to convert categorical data into binary format again making it more suitable to train the neural network. Then, I split the training dataset into training and cross-validation to evaluate the performance of the dataset. Then, I imported various modules from Keras - a deep learning library. I have used all these steps to build a convolutional neural network (well-suited for image recognition) with 3 convolutional layers, followed by two full-connected layers and finally an output layer. This approach has achieved my best score of 0. 992 in the Digit Recognizer problem. 

# Supervised ML Implementation
Moving on to the supervised machine learning approach, I have first found out that we have 784 independent variables and 1 dependent variable (label) of the digit in our train dataset. This, in my opinion, qualifies as a high-dimensional space which I want to transform into a lower dimensional space using dimensionality reduction techniques such as Principal Component Analysis, Linear Discriminant Analysis, or t-SNE. I have started with PCA implemented using Python and some specific libraries with this functionality. I standardized the data as PCA is sensitive to scale. I have tried different number of components (150, 100, 50, 30) and decided to stick with 150 as the first 150 principal components explain 0. 81 of the variances in the train dataset. After that I fit different algorithms to this transformed dataset which now consists of 150 principal components, instead of the original 784 variables. After I finished my PCA, I used the 150 principal components to train Random Forest, XGboost, Decision Tree, and Support Vector Machine algorithms. 

# Official Kaggle Scores
These are my official Kaggle scores for all the algorithms mentioned above:
XGBoost: 0. 950
Support Vector Machine: 0. 932
Random Forest: 0. 914

# Closing
Overall, I have learned a lot by working on this assignment – Digit Recognizer problem using the MNIST dataset. It all started with the realization that I really do not need to completely understand the dataset (this is different than working with visualizations) when working on implementing a supervised machine learning model – all I needed to do is use Principal Component Analysis to reduce the dimensionality of the dataset and then using the new – fewer variable dataset – feed the information into variable supervised machine learning algorithms and check their accuracy. From my work, XGBoost performed the best, however, there are other ways to solve this problem such as implementing a different dimension reduction technique (Linear Discriminant Analysis, or t-SNE) or still using Principal Component Analysis but with fewer or more components. This would definitely improve or worsen my score. 